{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kbF9QFCb_7zm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec8f6cd1-7053-4023-9bd9-e92695d86bba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas beautifulsoup4 nltk scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# to get dataset from google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0LFUa4wN347f",
        "outputId": "c9541be9-4e04-44ba-d189-14d85b435bbb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Make variables\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Function to clean the IMdb Dataset\n",
        "def clean_text(review):\n",
        "    # Cleaning HTML tags\n",
        "    review = BeautifulSoup(review, 'html.parser').get_text()\n",
        "    # Removing Special Characters\n",
        "    review = re.sub(\"[^a-zA-Z]\", \" \", review)\n",
        "    # Converting Reviews to lowercase\n",
        "    review = review.lower()\n",
        "    # Tokenizing reviews\n",
        "    words = nltk.word_tokenize(review)\n",
        "    # Removing stopwords and lemmatizing\n",
        "    foundWords= [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
        "    cleanReviws = ' '.join(foundWords)\n",
        "    return cleanReviws\n",
        "\n",
        "# Function to preprocess data and return cleaned data\n",
        "def preprocess_data(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    df['cleanReviws'] = df['review'].apply(clean_text)\n",
        "    return df[['cleanReviws']]\n",
        "\n",
        "# Function to vectorize the data using TF-IDF\n",
        "def vectorize_data(df):\n",
        "    # max_features will take the top specified words from the total vectorized words\n",
        "    vectorize = TfidfVectorizer(max_features=5000)\n",
        "    X = vectorize.fit_transform(df['cleanReviws']).toarray()\n",
        "    return X, vectorize\n",
        "\n",
        "# File path to dataset (in google drive)\n",
        "file_path = '/content/drive/My Drive/Colab Notebooks/IMDB Dataset.csv'\n",
        "\n",
        "# Preprocess the data\n",
        "df = preprocess_data(file_path)\n",
        "\n",
        "# Print the first 10 cleaned reviews\n",
        "print(df['cleanReviws'].head(10))\n",
        "\n",
        "# Clean Reviews saved in new file\n",
        "output_path = '/content/drive/My Drive/Colab Notebooks/dataCleaned.csv'\n",
        "df.to_csv(output_path, index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N76yJg0TNfIz",
        "outputId": "769facad-0e29-4d86-d1e1-0c6e1132eb72"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "<ipython-input-2-eab9bbf1f534>:23: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  review = BeautifulSoup(review, 'html.parser').get_text()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    one reviewer mentioned watching oz episode hoo...\n",
            "1    wonderful little production filming technique ...\n",
            "2    thought wonderful way spend time hot summer we...\n",
            "3    basically family little boy jake think zombie ...\n",
            "4    petter mattei love time money visually stunnin...\n",
            "5    probably time favorite movie story selflessnes...\n",
            "6    sure would like see resurrection dated seahunt...\n",
            "7    show amazing fresh innovative idea first aired...\n",
            "8    encouraged positive comment film looking forwa...\n",
            "9    like original gut wrenching laughter like movi...\n",
            "Name: cleanReviws, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Make variables\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Function to clean the IMDB Dataset\n",
        "def clean_text(review):\n",
        "    # Cleaning HTML tags\n",
        "    review = BeautifulSoup(review, 'html.parser').get_text()\n",
        "    # Removing Special Characters\n",
        "    review = re.sub(\"[^a-zA-Z]\", \" \", review)\n",
        "    # Converting Reviews to lowercase\n",
        "    review = review.lower()\n",
        "    # Tokenizing reviews\n",
        "    words = nltk.word_tokenize(review)\n",
        "    # Removing stopwords and lemmatizing\n",
        "    foundWords = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
        "    cleanReviews = ' '.join(foundWords)\n",
        "    return cleanReviews\n",
        "\n",
        "# Function to preprocess data and return cleaned data\n",
        "def preprocess_data(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    df['cleanReviews'] = df['review'].apply(clean_text)\n",
        "    return df[['cleanReviews', 'sentiment']]\n",
        "\n",
        "# File path to dataset (in Google Drive)\n",
        "file_path = '/content/drive/My Drive/Colab Notebooks/IMDB Dataset.csv'\n",
        "\n",
        "# Preprocess the data\n",
        "df = preprocess_data(file_path)\n",
        "\n",
        "# Print the first 10 cleaned reviews\n",
        "print(df.head(10))\n",
        "\n",
        "# Save cleaned reviews and sentiment to a new file\n",
        "output_path = '/content/drive/My Drive/Colab Notebooks/dataCleaned.csv'\n",
        "df.to_csv(output_path, index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d17_9SMi0Uxi",
        "outputId": "161b1065-8a7c-4971-c411-cda39cd4b768"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "<ipython-input-3-db7fbe560923>:22: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  review = BeautifulSoup(review, 'html.parser').get_text()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                        cleanReviews sentiment\n",
            "0  one reviewer mentioned watching oz episode hoo...  positive\n",
            "1  wonderful little production filming technique ...  positive\n",
            "2  thought wonderful way spend time hot summer we...  positive\n",
            "3  basically family little boy jake think zombie ...  negative\n",
            "4  petter mattei love time money visually stunnin...  positive\n",
            "5  probably time favorite movie story selflessnes...  positive\n",
            "6  sure would like see resurrection dated seahunt...  positive\n",
            "7  show amazing fresh innovative idea first aired...  negative\n",
            "8  encouraged positive comment film looking forwa...  negative\n",
            "9  like original gut wrenching laughter like movi...  positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing libraries for model training\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import pandas as pd\n",
        "\n",
        "# Loading the cleaned reviews\n",
        "file_path = '/content/drive/My Drive/Colab Notebooks/dataCleaned.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Splitting the dataset into features and target variable\n",
        "X = df['cleanReviews']\n",
        "y = df['sentiment']\n",
        "\n",
        "# Using TF-IDF Vectorizer for feature extraction\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(X)\n",
        "\n",
        "# Splitting the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=3000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(f'Accuracy of Model: {accuracy:.3f}')\n",
        "print(f'Precision of Model: {precision:.3f}')\n",
        "print(f'Recall: {recall:.3f}')\n",
        "print(f'F1 Score: {f1:.3f}')\n",
        "\n",
        "# Testing the model with sample sentences\n",
        "sample_sentences = [\"The stunning visuals and compelling storyline make this movie a must-watch for any cinema lover.\",\n",
        "                     \"The pacing was incredibly slow, making it hard to stay engaged with the story.\",\n",
        "                     \"This film masterfully blends humor and drama, leaving the audience both entertained and moved.\"]\n",
        "\n",
        "# Vectorizing the sample sentences\n",
        "sample_tfidf = tfidf_vectorizer.transform(sample_sentences)\n",
        "\n",
        "# Sentiment Prediction\n",
        "sample_predictions = model.predict(sample_tfidf)\n",
        "\n",
        "# Printing Prediction\n",
        "for sentence, sentiment in zip(sample_sentences, sample_predictions):\n",
        "    print(f'Sentence: \"{sentence}\" - Sentiment: {sentiment}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHSICBLexqZS",
        "outputId": "beada0f3-254c-4627-b9b4-c8f4cd86eb45"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Model: 0.888\n",
            "Precision of Model: 0.888\n",
            "Recall: 0.888\n",
            "F1 Score: 0.888\n",
            "Sentence: \"The stunning visuals and compelling storyline make this movie a must-watch for any cinema lover.\" - Sentiment: positive\n",
            "Sentence: \"The pacing was incredibly slow, making it hard to stay engaged with the story.\" - Sentiment: negative\n",
            "Sentence: \"This film masterfully blends humor and drama, leaving the audience both entertained and moved.\" - Sentiment: positive\n"
          ]
        }
      ]
    }
  ]
}